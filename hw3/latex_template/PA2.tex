\documentclass[11pt,addpoints,answers]{exam}

%-----------------------------------------------------------------------------
% PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%-----------------------------------------------------------------------------

\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsfonts}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{titling}
\usepackage{url}
\usepackage{xfrac}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{paralist}
\usepackage{epstopdf}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[colorlinks=true,urlcolor=blue]{hyperref}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{float}
\usepackage{enumerate}
\usepackage{array}
\usepackage{environ}
\usepackage{times}
\usepackage{textcomp}
\usepackage{caption}
\usepackage{parskip} % For NIPS style paragraphs.
\usepackage[compact]{titlesec} % Less whitespace around titles
\usepackage[inline]{enumitem} % For inline enumerate* and itemize*
\usepackage{datetime}
\usepackage{comment}
% \usepackage{minted}
\usepackage{lastpage}
\usepackage{color}
\usepackage{xcolor}
\usepackage[final]{listings}
\usepackage{tikz}
\usetikzlibrary{shapes,decorations}
\usepackage{framed}
\usepackage{booktabs}
\usepackage{cprotect}
\usepackage{verbatim}
\usepackage{verbatimbox}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{mathtools} % For drcases
\usepackage{cancel}
\usepackage[many]{tcolorbox}
\usepackage{soul}
\usepackage[bottom]{footmisc}
\usepackage{bm}
\usepackage{wasysym}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{positioning, arrows, automata, calc}
\usepackage{transparent}

\newtcolorbox[]{your_solution}[1][]{
    % breakable,
    enhanced,
    nobeforeafter,
    colback=white,
    title=Your Answer,
    sidebyside align=top,
    box align=top,
    #1
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Formatting for \CorrectChoice of "exam" %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\CorrectChoiceEmphasis{}
\checkedchar{\blackcircle}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Rotated Column Headers                  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{adjustbox}
\usepackage{array}

%https://tex.stackexchange.com/questions/32683/rotated-column-titles-in-tabular

\newcolumntype{R}[2]{%
    >{\adjustbox{angle=#1,lap=\width-(#2)}\bgroup}%
    l%
    <{\egroup}%
}
\newcommand*\rot{\multicolumn{1}{R{45}{1em}}}% no optional argument here, please!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Custom commands                        %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\vc}[1]{\boldsymbol{#1}}
\newcommand{\adj}[1]{\frac{d J}{d #1}}
\newcommand{\chain}[2]{\adj{#2} = \adj{#1}\frac{d #1}{d #2}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\blackcircle}{\tikz\draw[black,fill=black] (0,0) circle (1ex);}
\renewcommand{\circle}{\tikz\draw[black] (0,0) circle (1ex);}

\newcommand{\emptysquare}{{\LARGE $\square$}\ \ }
\newcommand{\filledsquare}{{\LARGE $\blacksquare$}\ \ }
\newcommand{\emptycircle}{{\LARGE $\fullmoon$}\ \ }
\newcommand{\filledcircle}{{\LARGE $\newmoon$}\ \ }

\newcommand{\ntset}{test}

% mathcal
\newcommand{\Ac}{\mathcal{A}}
\newcommand{\Bc}{\mathcal{B}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\Dc}{\mathcal{D}}
\newcommand{\Ec}{\mathcal{E}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Gc}{\mathcal{G}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\Ic}{\mathcal{I}}
\newcommand{\Jc}{\mathcal{J}}
\newcommand{\Kc}{\mathcal{K}}
\newcommand{\Lc}{\mathcal{L}}
\newcommand{\Mc}{\mathcal{M}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\Oc}{\mathcal{O}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Qc}{\mathcal{Q}}
\newcommand{\Rc}{\mathcal{R}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\Tc}{\mathcal{T}}
\newcommand{\Uc}{\mathcal{U}}
\newcommand{\Vc}{\mathcal{V}}
\newcommand{\Wc}{\mathcal{W}}
\newcommand{\Xc}{\mathcal{X}}
\newcommand{\Yc}{\mathcal{Y}}
\newcommand{\Zc}{\mathcal{Z}}

% mathbb
\newcommand{\Ab}{\mathbb{A}}
\newcommand{\Bb}{\mathbb{B}}
\newcommand{\Cb}{\mathbb{C}}
\newcommand{\Db}{\mathbb{D}}
\newcommand{\Eb}{\mathbb{E}}
\newcommand{\Fb}{\mathbb{F}}
\newcommand{\Gb}{\mathbb{G}}
\newcommand{\Hb}{\mathbb{H}}
\newcommand{\Ib}{\mathbb{I}}
\newcommand{\Jb}{\mathbb{J}}
\newcommand{\Kb}{\mathbb{K}}
\newcommand{\Lb}{\mathbb{L}}
\newcommand{\Mb}{\mathbb{M}}
\newcommand{\Nb}{\mathbb{N}}
\newcommand{\Ob}{\mathbb{O}}
\newcommand{\Pb}{\mathbb{P}}
\newcommand{\Qb}{\mathbb{Q}}
\newcommand{\Rb}{\mathbb{R}}
\newcommand{\Sb}{\mathbb{S}}
\newcommand{\Tb}{\mathbb{T}}
\newcommand{\Ub}{\mathbb{U}}
\newcommand{\Vb}{\mathbb{V}}
\newcommand{\Wb}{\mathbb{W}}
\newcommand{\Xb}{\mathbb{X}}
\newcommand{\Yb}{\mathbb{Y}}
\newcommand{\Zb}{\mathbb{Z}}

% mathbf lowercase
\newcommand{\av}{\mathbf{a}}
\newcommand{\bv}{\mathbf{b}}
\newcommand{\cv}{\mathbf{c}}
\newcommand{\dv}{\mathbf{d}}
\newcommand{\ev}{\mathbf{e}}
\newcommand{\fv}{\mathbf{f}}
\newcommand{\gv}{\mathbf{g}}
\newcommand{\hv}{\mathbf{h}}
\newcommand{\iv}{\mathbf{i}}
\newcommand{\jv}{\mathbf{j}}
\newcommand{\kv}{\mathbf{k}}
\newcommand{\lv}{\mathbf{l}}
\newcommand{\mv}{\mathbf{m}}
\newcommand{\nv}{\mathbf{n}}
\newcommand{\ov}{\mathbf{o}}
\newcommand{\pv}{\mathbf{p}}
\newcommand{\qv}{\mathbf{q}}
\newcommand{\rv}{\mathbf{r}}
\newcommand{\sv}{\mathbf{s}}
\newcommand{\tv}{\mathbf{t}}
\newcommand{\uv}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\wv}{\mathbf{w}}
\newcommand{\xv}{\mathbf{x}}
\newcommand{\yv}{\mathbf{y}}
\newcommand{\zv}{\mathbf{z}}

% mathbf uppercase
\newcommand{\Av}{\mathbf{A}}
\newcommand{\Bv}{\mathbf{B}}
\newcommand{\Cv}{\mathbf{C}}
\newcommand{\Dv}{\mathbf{D}}
\newcommand{\Ev}{\mathbf{E}}
\newcommand{\Fv}{\mathbf{F}}
\newcommand{\Gv}{\mathbf{G}}
\newcommand{\Hv}{\mathbf{H}}
\newcommand{\Iv}{\mathbf{I}}
\newcommand{\Jv}{\mathbf{J}}
\newcommand{\Kv}{\mathbf{K}}
\newcommand{\Lv}{\mathbf{L}}
\newcommand{\Mv}{\mathbf{M}}
\newcommand{\Nv}{\mathbf{N}}
\newcommand{\Ov}{\mathbf{O}}
\newcommand{\Pv}{\mathbf{P}}
\newcommand{\Qv}{\mathbf{Q}}
\newcommand{\Rv}{\mathbf{R}}
\newcommand{\Sv}{\mathbf{S}}
\newcommand{\Tv}{\mathbf{T}}
\newcommand{\Uv}{\mathbf{U}}
\newcommand{\Vv}{\mathbf{V}}
\newcommand{\Wv}{\mathbf{W}}
\newcommand{\Xv}{\mathbf{X}}
\newcommand{\Yv}{\mathbf{Y}}
\newcommand{\Zv}{\mathbf{Z}}

% bold greek lowercase
\newcommand{\alphav     }{\boldsymbol \alpha     }
\newcommand{\betav      }{\boldsymbol \beta      }
\newcommand{\gammav     }{\boldsymbol \gamma     }
\newcommand{\deltav     }{\boldsymbol \delta     }
\newcommand{\epsilonv   }{\boldsymbol \epsilon   }
\newcommand{\varepsilonv}{\boldsymbol \varepsilon}
\newcommand{\zetav      }{\boldsymbol \zeta      }
\newcommand{\etav       }{\boldsymbol \eta       }
\newcommand{\thetav     }{\boldsymbol \theta     }
\newcommand{\varthetav  }{\boldsymbol \vartheta  }
\newcommand{\iotav      }{\boldsymbol \iota      }
\newcommand{\kappav     }{\boldsymbol \kappa     }
\newcommand{\varkappav  }{\boldsymbol \varkappa  }
\newcommand{\lambdav    }{\boldsymbol \lambda    }
\newcommand{\muv        }{\boldsymbol \mu        }
\newcommand{\nuv        }{\boldsymbol \nu        }
\newcommand{\xiv        }{\boldsymbol \xi        }
\newcommand{\omicronv   }{\boldsymbol \omicron   }
\newcommand{\piv        }{\boldsymbol \pi        }
\newcommand{\varpiv     }{\boldsymbol \varpi     }
\newcommand{\rhov       }{\boldsymbol \rho       }
\newcommand{\varrhov    }{\boldsymbol \varrho    }
\newcommand{\sigmav     }{\boldsymbol \sigma     }
\newcommand{\varsigmav  }{\boldsymbol \varsigma  }
\newcommand{\tauv       }{\boldsymbol \tau       }
\newcommand{\upsilonv   }{\boldsymbol \upsilon   }
\newcommand{\phiv       }{\boldsymbol \phi       }
\newcommand{\varphiv    }{\boldsymbol \varphi    }
\newcommand{\chiv       }{\boldsymbol \chi       }
\newcommand{\psiv       }{\boldsymbol \psi       }
\newcommand{\omegav     }{\boldsymbol \omega     }

% bold greek uppercase
\newcommand{\Gammav     }{\boldsymbol \Gamma     }
\newcommand{\Deltav     }{\boldsymbol \Delta     }
\newcommand{\Thetav     }{\boldsymbol \Theta     }
\newcommand{\Lambdav    }{\boldsymbol \Lambda    }
\newcommand{\Xiv        }{\boldsymbol \Xi        }
\newcommand{\Piv        }{\boldsymbol \Pi        }
\newcommand{\Sigmav     }{\boldsymbol \Sigma     }
\newcommand{\Upsilonv   }{\boldsymbol \Upsilon   }
\newcommand{\Phiv       }{\boldsymbol \Phi       }
\newcommand{\Psiv       }{\boldsymbol \Psi       }
\newcommand{\Omegav     }{\boldsymbol \Omega     }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Code highlighting with listings         %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\definecolor{bluekeywords}{rgb}{0.13,0.13,1}
\definecolor{greencomments}{rgb}{0,0.5,0}
\definecolor{redstrings}{rgb}{0.9,0,0}
\definecolor{light-gray}{gray}{0.95}

\newcommand{\MYhref}[3][blue]{\href{#2}{\color{#1}{#3}}}%

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstdefinelanguage{Shell}{
  keywords={tar, cd, make},
  %keywordstyle=\color{bluekeywords}\bfseries,
  alsoletter={+},
  ndkeywords={python, py, javac, java, gcc, c, g++, cpp, .txt, octave, m, .tar},
  %ndkeywordstyle=\color{bluekeywords}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  %stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]",
  backgroundcolor = \color{light-gray}
}

\lstset{columns=fixed, basicstyle=\ttfamily,
    backgroundcolor=\color{light-gray},xleftmargin=0.5cm,frame=tlbr,framesep=4pt,framerule=0pt}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Custom box for highlights               %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Define box and box title style
\tikzstyle{mybox} = [fill=blue!10, very thick,
    rectangle, rounded corners, inner sep=1em, inner ysep=1em]

% \newcommand{\notebox}[1]{
% \begin{tikzpicture}
% \node [mybox] (box){%
%     \begin{minipage}{\textwidth}
%     #1
%     \end{minipage}
% };
% \end{tikzpicture}%
% }

\NewEnviron{notebox}{
\begin{tikzpicture}
\node [mybox] (box){
    \begin{minipage}{\textwidth}
        \BODY
    \end{minipage}
};
\end{tikzpicture}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Commands showing / hiding solutions     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% To HIDE SOLUTIONS (to post at the website for students), set this value to 0: 
%\def\issoln{0}
% Some commands to allow solutions to be embedded in the assignment file.
\ifcsname issoln\endcsname \else \def\issoln{1} \fi
% Default to an empty solutions environ.
\NewEnviron{soln}{}{}
\if\issoln 1
% Otherwise, include solutions as below.
\RenewEnviron{soln}{
    \leavevmode\color{red}\ignorespaces
    % \textbf{Solution} \BODY
    \BODY
}{}
\fi

%% qauthor environment:
% Default to an empty qauthor environ.
\NewEnviron{qauthor}{}{}
%% To HIDE TAGS set this value to 0:
\def\showtags{0}
%%%%%%%%%%%%%%%%
\ifcsname showtags\endcsname \else \def\showtags{1} \fi
% Default to an empty tags environ.
\NewEnviron{tags}{}{}
\if\showtags 1
% Otherwise, include solutions as below.
\RenewEnviron{tags}{
    \fbox{
    \leavevmode\color{blue}\ignorespaces
    \textbf{TAGS:} \texttt{\url{\BODY}}
    }
    \vspace{-.5em}
}{}
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Commands for customizing the assignment %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\courseName}{10-301/10-601 Introduction to Machine Learning (Summer 2023)}
\newcommand{\hwName}{Programming Assignment 2: $k$NN \& Model Selection}
\newcommand{\dueDate}{Thursday, June 1st}

\title{\textsc{\hwName}
%\thanks{Compiled on \today{} at \currenttime{}}
} % Title

\author{\courseName\\
\url{https://www.cs.cmu.edu/~hchai2/courses/10601/} \\
OUT: Thursday, May 25th \\
DUE: \dueDate{} \\ 
TAs: Alex, Andrew, Sofia, Tara, Markov, Neural the Narwhal
}

\newcommand{\homeworktype}{\string written/programming}

\date{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Useful commands for typesetting the questions %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand \expect {\mathbb{E}}
\newcommand \mle [1]{{\hat #1}^{\rm MLE}}
\newcommand \map [1]{{\hat #1}^{\rm MAP}}
\newcommand \argmax {\operatorname*{argmax}}
\newcommand \argmin {\operatorname*{argmin}}
\newcommand \code [1]{{\tt #1}}
\newcommand \datacount [1]{\#\{#1\}}
\newcommand \ind [1]{\mathbb{I}\{#1\}}

%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document configuration %
%%%%%%%%%%%%%%%%%%%%%%%%%%

% Don't display a date in the title and remove the white space
\predate{}
\postdate{}
\date{}

% Don't display an author and remove the white space
%\preauthor{}
%\postauthor{}

% Solo and group questions
\newcommand{\solo}{\textbf{[SOLO]} }
\newcommand{\group}{\textbf{[GROUP]} }

% Question type commands
\newcommand{\sall}{\textbf{Select all that apply: }}
\newcommand{\sone}{\textbf{Select one: }}
\newcommand{\tf}{\textbf{True or False: }}

% AdaBoost commands
\newcommand{\trainerr}[1]{\hat{\epsilon}_S \left(#1\right)}
\newcommand{\generr}[1]{\epsilon \left(#1\right)}
\newcommand{\D}{\mathcal{D}}
\newcommand{\margin}{\text{margin}}
\newcommand{\sign}{\text{sign}}
\newcommand{\PrS}{\hat{\Pr_{(x_i, y_i) \sim S}}}
\newcommand{\PrSinline}{\hat{\Pr}_{(x_i, y_i) \sim S}}  % inline PrS

% Abhi messing around with examdoc
\qformat{\textbf{{\Large \thequestion \; \; \thequestiontitle \ (\totalpoints \ points)}} \hfill}
\renewcommand{\thequestion}{\arabic{question}}
\renewcommand{\questionlabel}{\thequestion.}

\renewcommand{\thepartno}{\arabic{partno}}
\renewcommand{\partlabel}{\thepartno.}
\renewcommand{\partshook}{\setlength{\leftmargin}{0pt}}

\renewcommand{\thesubpart}{\alph{subpart}}
\renewcommand{\subpartlabel}{(\thesubpart)}

\renewcommand{\thesubsubpart}{\roman{subsubpart}}
\renewcommand{\subsubpartlabel}{\thesubsubpart.}

% copied from stack overflow, as all good things are
\newcommand\invisiblesection[1]{%
  \refstepcounter{section}%
  \addcontentsline{toc}{section}{\protect\numberline{\thesection}#1}%
  \sectionmark{#1}}

% quite possibly the worst workaround i have made for this class
\newcommand{\sectionquestion}[1]{
\titledquestion{#1}
\invisiblesection{#1}
~\vspace{-1em}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% New Environment for Pseudocode          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Python style for highlighting
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
morekeywords={self},              % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false
}}


% Python environment
\lstnewenvironment{your_code_solution}[1][]
{
\pythonstyle
\lstset{#1}
}
{}


%%%%%%%%%%%%%%%%%%
% Begin Document %
%%%%%%%%%%%%%%%%%% 

\begin{document}

\maketitle

\vspace*{-6mm}
\begin{notebox}
\paragraph{Summary} In this assignment, you will build a $k$NN classifier and apply it to several binary classification problems, with the goal finding the best value of $k$ using both validation and cross-validation. You will implement $k$NN models using  Euclidean distance and Manhattan distance, plot training and (cross-)validation error rates as a function of $k$, and finally compare the effect of ``re-training'' your models on both the training \emph{and} validation datasets as opposed to just using the training dataset. 
\end{notebox}
\vspace*{-5mm}\section*{START HERE: Instructions}
\begin{itemize}
\item \textbf{Collaboration policy:} Collaboration on solving the homework is allowed, after you have thought about the problems on your own. It is also OK to get clarification (but not solutions) from books or online resources, again after you have thought about the problems on your own. There are two requirements: first, cite your collaborators fully and completely (e.g., ``Jane explained to me what is asked in Question 2.1''). Second, write your solution {\em independently}: close the book and all of your notes, and send collaborators out of the room, so that the solution comes from you only.  See the Academic Integrity Section on the course site for more information: \url{https://www.cs.cmu.edu/~hchai2/courses/10601/}

\item\textbf{Late Submission Policy:} See the late submission policy here: \url{https://www.cs.cmu.edu/~hchai2/courses/10601/}

\item\textbf{Submitting your work:} 

\begin{itemize}

% Since we are not using Canvas this semester.
% \item \textbf{Canvas:} We will use an online system called Canvas for short answer and multiple choice questions. You can log in with your Andrew ID and password. (As a reminder, never enter your Andrew password into any website unless you have first checked that the URL starts with "https://" and the domain name ends in ".cmu.edu" -- but in this case it's OK since both conditions are met).  You may only \textbf{submit once} on canvas, so be sure of your answers before you submit.  However, canvas allows you to work on your answers and then close out of the page and it will save your progress.  You will not be granted additional submissions, so please be confident of your solutions when you are submitting your assignment.

\item \textbf{Programming:} You will submit your code for programming questions on the homework to Gradescope (\url{https://gradescope.com}). After uploading your code, our grading scripts will autograde your assignment by running your program on a virtual machine (VM). When you are developing, check that the version number of the programming language environment (e.g. Python 3.9.12) and versions of permitted libraries (e.g.  \texttt{numpy} 1.23.0) match those used on Gradescope. You have a \textbf{total of 10 Gradescope programming submissions.} Use them wisely. In order to not waste code submissions, we recommend debugging your implementation on your local machine (or the linux servers) and making sure your code is running correctly first before any Gradescope coding submission.

\item \textbf{Written:} For written problems such as short answer, multiple choice, derivations, proofs, or plots, please use the provided template. You must typeset your submission using \LaTeX{}. If your submission is misaligned with the template, there will be a \textbf{\textcolor{red}{2\% penalty}} (e.g., if the homework is out of 100 points, 2 points will be deducted from your final score). Each derivation/proof should be completed in the boxes provided. Do not move or resize any of the answer boxes. If you do not follow the template, your assignment may not be graded correctly by our AI assisted grader.

\end{itemize}

\end{itemize}

%Homework 9 will be on Gradescope, but will be "Canvas-style"- all problems will be multiple choice, select all that apply, or numerical answer. 

For multiple choice or select all that apply questions, shade in the box or circle in the template document corresponding to the correct answer(s) for each of the questions. For \LaTeX{} users, replace \lstinline{\choice} with \lstinline{\CorrectChoice} to obtain a shaded box/circle, and don't change anything else.\clearpage

\section*{Instructions for Specific Problem Types}

For ``Select One" questions, please fill in the appropriate bubble completely:

\begin{quote}
\textbf{Select One:} Who taught this course?
    \begin{checkboxes}
     \CorrectChoice Henry Chai
     \choice Marie Curie
     \choice Noam Chomsky
    \end{checkboxes}
\end{quote}

If you need to change your answer, you may cross out the previous answer and bubble in the new answer:

\begin{quote}
\textbf{Select One:} Who taught this course?
    {
    \begin{checkboxes}
     \CorrectChoice Henry Chai
     \choice Marie Curie \checkboxchar{\xcancel{\blackcircle}{}}
     \choice Noam Chomsky
    \end{checkboxes}
    }
\end{quote}

For ``Select all that apply" questions, please fill in all appropriate squares completely:

\begin{quote}
\textbf{Select all that apply:} Which are scientists?
    {%
    \checkboxchar{$\Box$} \checkedchar{$\blacksquare$} % change checkbox style locally
    \begin{checkboxes}
    \CorrectChoice Stephen Hawking 
    \CorrectChoice Albert Einstein
    \CorrectChoice Isaac Newton
    \choice I don't know
    \end{checkboxes}
    }
\end{quote}

Again, if you need to change your answer, you may cross out the previous answer(s) and bubble in the new answer(s):

\begin{quote}
\textbf{Select all that apply:} Which are scientists?
    {%
    \checkboxchar{\xcancel{$\blacksquare$}} \checkedchar{$\blacksquare$} % change checkbox style locally
    \begin{checkboxes}
    \CorrectChoice Stephen Hawking 
    \CorrectChoice Albert Einstein
    \CorrectChoice Isaac Newton
    \choice I don't know
    \end{checkboxes}
    }
\end{quote}

For questions where you must fill in a blank, please make sure your final answer is fully included in the given space. You may cross out answers or parts of answers, but the final answer must still be within the given space.

\begin{quote}
\textbf{Fill in the blank:} What is the course number?

\begin{tcolorbox}[fit,height=1cm, width=4cm, blank, borderline={1pt}{-2pt},nobeforeafter]
    \begin{center}\huge10-601\end{center}
    \end{tcolorbox}\hspace{2cm}
    \begin{tcolorbox}[fit,height=1cm, width=4cm, blank, borderline={1pt}{-2pt},nobeforeafter]
    \begin{center}\huge10-\xcancel{6}301\end{center}
    \end{tcolorbox}
\end{quote}
\clearpage

{\LARGE \bf Written Problems (\numpoints \ points)}
\begin{questions}
\sectionquestion{Empirical Questions}
\label{sec:empQ}

The following questions should be completed as you work through the programming portion of this assignment.

 \begin{parts}
    \part[3] For the \texttt{iris} dataset, create a \emph{computer-generated} plot showing error rate on the y-axis against $k$ on the x-axis for $k$ going from $1$ to $90$ (the number of training data points). Use the Euclidean distance metric for this question. 
    
    On a single set of axes, include \emph{both} training error rate and validation error rate, clearly labeling which is which. That is, for each possible value of $k$ from $1$ to $90$, you should report the error rate of a $k$NN classifier's predictions on both the training dataset and validation datasets given to you. You should include an image file below using the provided, commented out code in \LaTeX{}, switching out \texttt{iris.png} to your file name as needed.
    
    \begin{your_solution}[title=Plot,height=15cm]
        % YOUR ANSWER 
        % \begin{center}
        % \includegraphics[width=0.5\linewidth]{iris.png}
        % % Change the "0.5\linewidth" part as necessary to make the picture fit in the box.
        % \end{center}
    \end{your_solution}

    \clearpage
    \part[4] Using the \texttt{iris} dataset, compare the effect of ``training'' your $k$NN model on just the training dataset vs. using both the training and validation datasets by computing the test error rate in both settings. Populate the table below for the pre-specified values of $k$. (Please round each number to the fourth decimal place, e.g. 0.1234)
    
    \textbf{NOTE:} you should \emph{absolutely} not do something like this in practice; comparing different values of $k$ should only be done using the validation dataset. If you were to use a table like this to inform your selection of $k$, your reported test error would no longer be a reliable estimate of your $k$NN classifier's true error; a good rule of thumb is that a test dataset should only be used \emph{once} in the entire pipeline. That being said, we are asking you to complete this task because it illustrates the fact that test error rate tends to decrease when models are trained on more data, regardless of the value of $k$.
    
    \begin{center}
        \begin{tabular}{c|c|c}
            {\bf $k$} & {\bf Test Error Rate (Train Only)} & {\bf Test Error Rate (Train + Validation)} \\
            \midrule
                1  &   &   \\
                20 &   &   \\
                40 &   &   \\
                60 &   &   \\
            \bottomrule
        \end{tabular}
    \end{center}
    
    
    \clearpage
    
    \part[3] For the \texttt{iris} dataset, create a \emph{computer-generated} plot showing $10$-fold cross-validation error on the y-axis against $k$ on the x-axis for $k$ going from $1$ to $90$ (the number of training data points). For this question, you will use both the Euclidean distance \emph{and} the Manhattan distance to compute the cross-validation errors. 
    
    On a single set of axes, include the $10$-fold cross-validation computed using \emph{both} the Euclidean distance and the Manhattan distance, clearly labeling which is which. That is, for each possible value of $k$ from $1$ to $90$, you should split the training dataset into $10$ equal-sized chunks or ``folds''. Then compute the average error rate when each fold is used as the validation dataset for a $k$NN model ``trained'' on the remaining $9$ folds with the Euclidean distance metric. Repeat this procedure with the Manhattan distance metric. You should include an image file below using the provided, commented out code in \LaTeX{}, switching out \texttt{crossval.png} to your file name as needed.
    
    \begin{your_solution}[title=Plot,height=15cm]
        % YOUR ANSWER 
        % \begin{center}
        % \includegraphics[width=0.5\linewidth]{crossval.png}
        % % Change the "0.5\linewidth" part as necessary to make the picture fit in the box.
        % \end{center}
    \end{your_solution}

    \clearpage
\end{parts}\clearpage
\newpage
\newpage
\section{Collaboration Questions}
After you have completed all other components of this assignment, report your answers to these questions regarding the collaboration policy. Details of the policy can be found \href{http://www.cs.cmu.edu/~mgormley/courses/10601/syllabus.html}{here}.
\begin{enumerate}
    \item Did you receive any help whatsoever from anyone in solving this assignment? If so, include full details.
    \item Did you give any help whatsoever to anyone in solving this assignment? If so, include full details.
    \item Did you find or come across code that implements any part of this assignment? If so, include full details.
\end{enumerate}

\begin{your_solution}[height=6cm]
% YOUR ANSWER 

\end{your_solution}
\newpage
\end{questions}

\section{Programming (88 points)}

Your goal in this assignment is to implement a classifier based on distance between points - specifically a $k$NN model. In addition, we will ask you to run some end-to-end experiments selecting the best value for $k$ using the classic Fisher iris dataset. 
%
You will write one program: \texttt{knn.py}. The program you write will be automatically graded using Gradescope.

\subsection{The Tasks and Datasets}
\label{sec:data}

\paragraph{Materials} Download the zip file from the course website. The zip file will have a handout folder that contains all the data that you will need in order to complete this assignment.

\paragraph{Starter Code} The handout will contain a preexisting \texttt{knn.py} file that itself contains some starter code for the assignment. While we do not require that you use the starter code in your final submission, we \emph{heavily} recommend building upon the structure laid out in the starter code.

\paragraph{Datasets}

The handout contains one dataset, which has already been split into training, validation and test datasets. The first line of each \lstinline{.csv} file contains the name of each attribute, and \emph{the class label is always the last column}.

The task associated with this dataset is to predict the species of an iris, based on physical measurements. Note that in this dataset, there are three possible labels: \emph{iris setosa} $(0)$, \emph{iris versicolor} $(1)$, and \emph{iris virginica} $(2)$. The attributes (aka. features) are: 
\begin{enumerate}
    \item \lstinline{sepal length}: The length of the sepal in cm.
    \item \lstinline{sepal width}: The width of the sepal in cm.
    \item \lstinline{petal length}: The length of the petals in cm.
    \item \lstinline{petal width}: The width of the petals in cm.
\end{enumerate}
The training data is in \lstinline{iris_train.csv}, the validation data is in \lstinline{iris_val.csv}, and the test data is in \lstinline{iris_test.csv}.

% \newpage
\subsection{Program \#1: $k$NN 88 points)}

In \texttt{knn.py}, you will implement a $k$ nearest neighbors classifier. 

\textbf{Your implementation must satisfy the following requirements:}
\begin{itemize}
\item Use Euclidian or Manhattan distance (as specified) to calculate distances between points.
\item Return predictions for a range of $k$ values as specified on the command line. 
\item Compute the error of a set of predictions given the true labels. 
\item Perform validation and $10$-fold cross-validation; \textbf{NOTE:} the number of folds is not a command line argument, you may hard code in a value of $10$ in your implementations.
\item Break ties in distance between training data points in favor of data points that appear \emph{earlier} in the training dataset i.e., ones with lower row indices. For example, if a data point is equally close to the 4th and the 10th training data point, then its nearest neighbor would be the 4th training data point.  
\item Break ties in labels of the nearest neighbors in favor of lower label values. For example, if the $5$ nearest neighbors to some data point have labels $0$, $0$, $1$, $2$, and $2$, then the predicted label would be $0$.
\item Do not hard-code any aspects of the datasets into your code. We may autograde your programs on hidden datasets that include different attributes and output labels.
\end{itemize}

Careful planning will help you to correctly and concisely implement your $k$NN model. Here are a few \emph{hints} to get you started:
\begin{itemize}
    \item Write and test helper functions to calculate the two distance metrics.
    \item Be sure to correctly handle the case where the specified number of neighbors is greater than the total number of points.
    \item When calling the cross-validation method, you should combine the training and validation datasets. 
    \item Also be sure to correctly handle the case where the number of folds does not divide evenly into the number of data points. 
    \item Look under the FAQ post on Piazza for more useful clarifications about the assignment.
\end{itemize}

\subsection{Command Line Arguments}

The autograder runs and evaluates the output from the files  generated, using the following command:

\begin{tabular}{ll}
\begin{lstlisting}[language=Shell]
$ python knn.py [args...]
\end{lstlisting}
\end{tabular}

Where above \lstinline{[args...]} is a placeholder for eleven command-line arguments, described in detail below:
\begin{enumerate}
\item \lstinline{<train input>}: path to the training input \lstinline{.csv} file (see Section \ref{sec:data})
\item \lstinline{<val input>}: path to the validation input \lstinline{.csv} file (see Section \ref{sec:data})
\item \lstinline{<test input>}: path to the test input \lstinline{.csv} file (see Section \ref{sec:data})
\item \lstinline{<val type>}: whether we use ``normal'' validation (0) or cross-validation (1). 
\item \lstinline{<dist metric>}: whether we use Euclidean (0) or Manhattan (1) distance. 
\item \lstinline{<min k>}: The smallest value of $k$ to consider.
\item \lstinline{<max k>}: The largest value of $k$ to consider.
\item \lstinline{<train out>}: path of output \lstinline{.txt} file to which the predictions on the \textit{training} data should be written (see Section \ref{sec:labels})
\item \lstinline{<val out>}: path of output \lstinline{.txt} file to which the predictions on the \textit{validation} data should be written (see Section \ref{sec:labels})
\item \lstinline{<test out>}: path of output \lstinline{.txt} file to which the predictions on the \emph{test} data should be written (see Section \ref{sec:labels})
\item \lstinline{<metrics out>}: path of the output \lstinline{.txt} file to which metrics such as train and test error should be written (see Section \ref{sec:metrics})
\end{enumerate}

As an example, the following command line would run your program on the iris dataset and learn a $k$NN classifier with the Manhattan distance metric, using $10$-fold cross-validation to find the best of $k$ from $10$ to $20$. The train predictions would be written to \lstinline{iris_10to20_train.txt}, the validation predictions to \lstinline{iris_10to20_val.txt}, the test predictions to \lstinline{iris_10to20_test.txt}, and the metrics to \lstinline{iris_10to20_metrics.txt}.
%
\begin{lstlisting}[language=Shell]
$ python knn.py iris_train.csv iris_val.csv iris_test.csv 1 1 10 20 \
iris_10to20_train.txt iris_10to20_val.txt iris_10to20_test.txt \ 
iris_10to20_metrics.txt
\end{lstlisting}

\subsection{Output: Labels Files}
\label{sec:labels}

Your program should write \emph{either two or three} output \lstinline{.txt} files depending on the \lstinline{<val type>} argument. 
\begin{enumerate}
    \item If \lstinline{<val type>} is $0$ (``normal'' validation), then you should output the predictions of your model on the training dataset (\lstinline{<train out>}), validation dataset (\lstinline{<val out>}), and test dataset (\lstinline{<test out>}). \lstinline{<train out>} and \lstinline{<val out>} should contain an array where the rows correspond to the input values of $k$ and the columns correspond to the predicted labels for each data point. \lstinline{<test out>} should contain a single list of predictions on the test dataset corresponding to the best value of $k$ i.e., the one with the lowest validation error rate. Columns in a row should be comma separated. Use `\texttt{\textbackslash n}' to create a new line.

    For example, if you set \lstinline{<min k>} to $10$ and \lstinline{<max k>} to $20$, then \lstinline{<train out>} would have $11$ rows and $90$ columns (as there are $90$ training data points); the 3rd entry in the 2nd row would be the prediction of an $11$NN model on the 3rd training data point. 
    
    The first few rows and columns of a sample \lstinline{<train out>} output file are given below for \lstinline{<val type>} $0$, where $k$ ranges from $1$ to $5$ and the Euclidean distance metric is used:
    \begin{quote}
        \begin{verbatim}
            2,1,2,0,...
            2,1,1,0,...
            2,1,2,0,...
            ...
        \end{verbatim}
    \end{quote}
    
    \item If \lstinline{<val type>} is $1$ ($10$-fold cross-validation), then you should output the predictions of your model on each fold of the combined training and validation datasets when that fold is held out (\lstinline{<val out>}) as well as the predictions on the test dataset (\lstinline{<test out>}). \lstinline{<val out>} should contain an array where the rows correspond to the input values of $k$ and the columns correspond to the predicted labels for each data point when that data point is not used to train the model. \lstinline{<test out>} should contain a single list of predictions on the test dataset corresponding to the best value of $k$ i.e., the one with the lowest cross-validation error rate. Columns in a row should be comma separated. Use `\texttt{\textbackslash n}' to create a new line.

    For example, if you set \lstinline{<min k>} to $10$ and \lstinline{<max k>} to $20$, then \lstinline{<val out>} would have $11$ rows and $120$ columns (as there are $120$ total training and validataion data points); the 3rd entry in the 2nd row would be the prediction of an $11$NN model on the 3rd data point when the last $108$ data points (the 2nd through 10th folds or all but the 1st fold) are used as the training dataset. 
    
    The first few rows of a sample \lstinline{<test out>} output file are given below for \lstinline{<val type>} $1$, where $k$ ranges from $1$ to $10$ and the Manhattan distance metric is used:
    \begin{quote}
        \begin{verbatim}
            1
            1
            0
            1
            ...
        \end{verbatim}
    \end{quote}
\end{enumerate}

Your labels should exactly match those of a reference $k$NN implementation --- this will be checked by the autograder by running your program and evaluating your output file against the reference solution.

\subsection{Output: Metrics File}
\label{sec:metrics}

Generate another file where you report the error rates for the various $k$NN classifiers your program implements. This file should be written to the path specified by the command line argument \lstinline{<metrics out>}. Again, the contents of this file will depend on \lstinline{<val type>}.

\begin{enumerate}
    \item If \lstinline{<val type>} is $0$ (``normal'' validation), then you should output the following error rates in this order: 
    \begin{itemize}
        \item the training error rates for each value of $k$ (each on their own line),
        \item the validation error rates for each value of $k$ 
        (again, each on their own line),
        \item the test error rate corresponding to just the best value of $k$ (the one that minimizes the validation error rate) \emph{where only the training dataset is used to make predictions}, and 
        \item the test error rate corresponding to just the best value of $k$ (the one that minimizes the validation error rate) \emph{where the training and validation datasets are first combined and then used to make predictions}.
    \end{itemize} 
    Use `\texttt{\textbackslash n}' to create a new line.
    
    A few lines of a sample \lstinline{<metrics out>} output file are given below for \lstinline{<val type>} $0$, where $k$ ranges from $1$ to $5$ and the Euclidean distance metric is used:
    %change this with the right answers
    \begin{quote}
    \begin{verbatim}
        k=1 training error rate: 0.0
        k=2 training error rate: 0.044444444444444446
        k=3 training error rate: 0.044444444444444446
        ...
        k=1 validation error rate: 0.06666666666666667
        k=2 validation error rate: 0.06666666666666667
        k=3 validation error rate: 0.03333333333333333
        ...
        test error rate (train): 0.0
        test error rate (train + validation): 0.0
    \end{verbatim}
    \end{quote}
    
    \item If \lstinline{<val type>} is $1$ ($10$-fold cross-validation), then you should output the $10$-fold cross-validation error rates for each value of $k$ (each on their own line) followed by the error rate for just the best value of $k$ (the one that minimizes the cross-validation error rate). Use `\texttt{\textbackslash n}' to create a new line.

    A few lines of a sample \lstinline{<metrics out>} output file are given below for \lstinline{<val type>} $1$, where $k$ ranges from $1$ to $10$ and the Manhattan distance metric is used:
    %change this with the right answers
    \begin{quote}
    \begin{verbatim}
        k=1 cross-validation error rate: 0.06666666666666667
        k=2 cross-validation error rate: 0.09166666666666666
        k=3 cross-validation error rate: 0.05
        ...
        test error rate: 0.0
    \end{verbatim}
    \end{quote}
\end{enumerate}

Again, your reported error rates should be within 0.0001 of the reference solution. You do not need to round your reported numbers! The autograder will automatically incorporate the right tolerance for float comparisons.

\begin{notebox}
At this point, you should be able to go back and answer questions 1-3 in the ``Empirical Questions" section of this handout.  Write your solutions in the template provided. \end{notebox}
    
\subsection{Submission Instructions}

\paragraph{Programming}
Please ensure you have completed the following file for submission.

\begin{verbatim}
knn.py
\end{verbatim}

When submitting your solution, make sure to select and upload the correct file. \textbf{Any other files will be deleted.} Ensure the file has the exact same spelling and letter casing as above. You can either directly zip the file (by selecting the file and compressing it -- do not compress the folder containing the file) or directly drag it to Gradescope for submission.

\paragraph{Written Questions}
Make sure you have completed all questions from Written component (including the collaboration policy questions) in the template provided.  When you have done so, please submit your document in \textbf{PDF format} to the corresponding assignment slot on Gradescope.\newpage

\end{document}